{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "This notebook is based on moondream's gaze detection.[link](https://github.com/vikhyat/moondream/tree/main/recipes/gaze-detection-video)\n",
        "\n"
      ],
      "metadata": {
        "id": "_oA6tM7TH9rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Dependencies.\n",
        "!sudo apt-get update && sudo apt-get install -y libvips42 libvips-dev ffmpeg"
      ],
      "metadata": {
        "id": "RKSKu8UK6RXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch>=2.0.0 transformers>=4.36.0 opencv-python>=4.8.0 pillow>=10.0.0 matplotlib>=3.7.0 numpy>=1.24.0 tqdm>=4.65.0 pyvips accelerate>=0.26.0 einops\n"
      ],
      "metadata": {
        "id": "CS9zti39Gu1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the File"
      ],
      "metadata": {
        "id": "lRPJ83hPIu0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "import os\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Specify the target directory where you want to move the file\n",
        "target_directory = 'input'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists(target_directory):\n",
        "    os.makedirs(target_directory)\n",
        "if not os.path.exists(target_directory):\n",
        "    os.makedirs('output')\n",
        "\n",
        "# Move the uploaded file to the target directory\n",
        "for filename in uploaded.keys():\n",
        "    shutil.move(filename, os.path.join(target_directory, filename))\n",
        "\n",
        "print(f\"File moved to: {target_directory}\")\n"
      ],
      "metadata": {
        "id": "e_6F8pF7IxrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Script.\n",
        "The output would be saved in an output named folder and also downloaded."
      ],
      "metadata": {
        "id": "XaXJttHiJ71u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Gaze Detection Video Processor using Moondream 2\n",
        "------------------------------------------------\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import glob\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from contextlib import contextmanager\n",
        "\n",
        "\n",
        "def initialize_model() -> Optional[AutoModelForCausalLM]:\n",
        "    \"\"\"Initialize the Moondream 2 model with error handling.\"\"\"\n",
        "    try:\n",
        "        print(\"\\nInitializing Moondream 2 model...\")\n",
        "        model_id = \"vikhyatk/moondream2\"\n",
        "        revision = \"2025-01-09\"  # Specify revision for stability\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
        "            device = \"cuda\"\n",
        "        else:\n",
        "            print(\"No GPU detected, using CPU\")\n",
        "            device = \"cpu\"\n",
        "\n",
        "        print(\"Loading model from HuggingFace...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            revision=revision,\n",
        "            trust_remote_code=True,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            low_cpu_mem_usage=True,\n",
        "            device_map={\"\": device} if device == \"cuda\" else None,\n",
        "        )\n",
        "\n",
        "        if device == \"cpu\":\n",
        "            model = model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        print(\"âœ“ Model initialized successfully\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError initializing model: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def video_handler(\n",
        "    input_path: str, output_path: str\n",
        ") -> Tuple[cv2.VideoCapture, cv2.VideoWriter]:\n",
        "    \"\"\"Context manager for handling video capture and writer.\"\"\"\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        raise ValueError(f\"Could not open video file: {input_path}\")\n",
        "\n",
        "    # Get video properties\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Create video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    try:\n",
        "        yield cap, out\n",
        "    finally:\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "def fig2rgb_array(fig: plt.Figure) -> np.ndarray:\n",
        "    \"\"\"Convert matplotlib figure to RGB array\"\"\"\n",
        "    fig.canvas.draw()\n",
        "    buf = fig.canvas.buffer_rgba()\n",
        "    w, h = fig.canvas.get_width_height()\n",
        "    img_array = np.asarray(buf).reshape((h, w, 4))\n",
        "    rgb_array = img_array[:, :, :3]  # Drop alpha channel\n",
        "    return rgb_array\n",
        "\n",
        "\n",
        "def visualize_frame(\n",
        "    frame: np.ndarray, faces: List[Dict], model: AutoModelForCausalLM, pil_image: Image\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Visualize a single frame using matplotlib\"\"\"\n",
        "    try:\n",
        "        # Create figure without margins\n",
        "        fig = plt.figure(figsize=(frame.shape[1] / 100, frame.shape[0] / 100), dpi=100)\n",
        "        ax = fig.add_axes([0, 0, 1, 1])\n",
        "\n",
        "        # Display frame\n",
        "        ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        # Sort faces by x_min coordinate for stable colors\n",
        "        faces = sorted(faces, key=lambda f: (f[\"y_min\"], f[\"x_min\"]))\n",
        "\n",
        "        # Generate colors\n",
        "        colors = plt.cm.rainbow(np.linspace(0, 1, max(1, len(faces))))\n",
        "\n",
        "        # Process each face\n",
        "        for face, color in zip(faces, colors):\n",
        "            try:\n",
        "                # Calculate face box coordinates\n",
        "                x_min = int(float(face[\"x_min\"]) * frame.shape[1])\n",
        "                y_min = int(float(face[\"y_min\"]) * frame.shape[0])\n",
        "                width = int(float(face[\"x_max\"] - face[\"x_min\"]) * frame.shape[1])\n",
        "                height = int(float(face[\"y_max\"] - face[\"y_min\"]) * frame.shape[0])\n",
        "\n",
        "                # Draw face rectangle\n",
        "                rect = plt.Rectangle(\n",
        "                    (x_min, y_min), width, height, fill=False, color=color, linewidth=2\n",
        "                )\n",
        "                ax.add_patch(rect)\n",
        "\n",
        "                # Calculate face center\n",
        "                face_center = (\n",
        "                    float(face[\"x_min\"] + face[\"x_max\"]) / 2,\n",
        "                    float(face[\"y_min\"] + face[\"y_max\"]) / 2,\n",
        "                )\n",
        "\n",
        "                # Try to detect gaze\n",
        "                try:\n",
        "                    gaze_result = model.detect_gaze(pil_image, face_center)\n",
        "                    if isinstance(gaze_result, dict) and \"gaze\" in gaze_result:\n",
        "                        gaze = gaze_result[\"gaze\"]\n",
        "                    else:\n",
        "                        gaze = gaze_result\n",
        "                except Exception as e:\n",
        "                    print(f\"Error detecting gaze: {e}\")\n",
        "                    continue\n",
        "\n",
        "                if (\n",
        "                    gaze is not None\n",
        "                    and isinstance(gaze, dict)\n",
        "                    and \"x\" in gaze\n",
        "                    and \"y\" in gaze\n",
        "                ):\n",
        "                    gaze_x = int(float(gaze[\"x\"]) * frame.shape[1])\n",
        "                    gaze_y = int(float(gaze[\"y\"]) * frame.shape[0])\n",
        "                    face_center_x = x_min + width // 2\n",
        "                    face_center_y = y_min + height // 2\n",
        "\n",
        "                    # Draw gaze line with gradient effect\n",
        "                    points = 50\n",
        "                    alphas = np.linspace(0.8, 0, points)\n",
        "\n",
        "                    # Calculate points along the line\n",
        "                    x_points = np.linspace(face_center_x, gaze_x, points)\n",
        "                    y_points = np.linspace(face_center_y, gaze_y, points)\n",
        "\n",
        "                    # Draw gradient line segments\n",
        "                    for i in range(points - 1):\n",
        "                        ax.plot(\n",
        "                            [x_points[i], x_points[i + 1]],\n",
        "                            [y_points[i], y_points[i + 1]],\n",
        "                            color=color,\n",
        "                            alpha=alphas[i],\n",
        "                            linewidth=4,\n",
        "                        )\n",
        "\n",
        "                    # Draw gaze point\n",
        "                    ax.scatter(gaze_x, gaze_y, color=color, s=100, zorder=5)\n",
        "                    ax.scatter(gaze_x, gaze_y, color=\"white\", s=50, zorder=6)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing face: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Configure axes\n",
        "        ax.set_xlim(0, frame.shape[1])\n",
        "        ax.set_ylim(frame.shape[0], 0)\n",
        "        ax.axis(\"off\")\n",
        "\n",
        "        # Convert matplotlib figure to image\n",
        "        frame_rgb = fig2rgb_array(fig)\n",
        "\n",
        "        # Convert RGB to BGR for OpenCV\n",
        "        frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Clean up\n",
        "        plt.close(fig)\n",
        "\n",
        "        return frame_bgr\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in visualize_frame: {e}\")\n",
        "        plt.close(\"all\")\n",
        "        return frame\n",
        "\n",
        "\n",
        "def process_video(\n",
        "    input_path: str, output_path: str, model: AutoModelForCausalLM\n",
        ") -> None:\n",
        "    \"\"\"Process video file and create new video with gaze visualization\"\"\"\n",
        "    with video_handler(input_path, output_path) as (cap, out):\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "        print(f\"Processing video: {total_frames} frames at {fps} FPS\")\n",
        "\n",
        "        # Process frames\n",
        "        with tqdm(\n",
        "            total=total_frames, desc=f\"Processing {os.path.basename(input_path)}\"\n",
        "        ) as pbar:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    # Convert frame for model\n",
        "                    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "                    # Detect faces\n",
        "                    detection_result = model.detect(pil_image, \"face\")\n",
        "\n",
        "                    # Handle different possible return formats\n",
        "                    if (\n",
        "                        isinstance(detection_result, dict)\n",
        "                        and \"objects\" in detection_result\n",
        "                    ):\n",
        "                        faces = detection_result[\"objects\"]\n",
        "                    elif isinstance(detection_result, list):\n",
        "                        faces = detection_result\n",
        "                    else:\n",
        "                        print(\n",
        "                            f\"Unexpected detection result format: {type(detection_result)}\"\n",
        "                        )\n",
        "                        faces = []\n",
        "\n",
        "                    # Ensure each face has the required coordinates\n",
        "                    faces = [\n",
        "                        face\n",
        "                        for face in faces\n",
        "                        if all(k in face for k in [\"x_min\", \"y_min\", \"x_max\", \"y_max\"])\n",
        "                    ]\n",
        "\n",
        "                    if not faces:\n",
        "                        processed_frame = frame\n",
        "                    else:\n",
        "                        # Visualize frame with matplotlib\n",
        "                        processed_frame = visualize_frame(\n",
        "                            frame, faces, model, pil_image\n",
        "                        )\n",
        "\n",
        "                    # Write frame\n",
        "                    out.write(processed_frame)\n",
        "                    pbar.update(1)\n",
        "\n",
        "                    # Force matplotlib to clean up\n",
        "                    plt.close(\"all\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing frame: {e}\")\n",
        "                    out.write(frame)  # Write original frame on error\n",
        "                    pbar.update(1)\n",
        "                    plt.close(\"all\")  # Clean up even on error\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Check if __file__ is defined (it won't be in interactive mode)\n",
        "    if '__file__' in globals():\n",
        "        base_dir = os.path.dirname(__file__)\n",
        "    else:\n",
        "        base_dir = os.getcwd()  # Use current working directory if running interactively\n",
        "\n",
        "    # Ensure input and output directories exist\n",
        "    input_dir = os.path.join(base_dir, \"input\")\n",
        "    output_dir = os.path.join(base_dir, \"output\")\n",
        "    os.makedirs(input_dir, exist_ok=True)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Find all video files in input directory\n",
        "    video_extensions = [\".mp4\", \".avi\", \".mov\", \".mkv\"]\n",
        "    input_videos = []\n",
        "    for ext in video_extensions:\n",
        "        input_videos.extend(glob.glob(os.path.join(input_dir, f\"*{ext}\")))\n",
        "\n",
        "    if not input_videos:\n",
        "        print(\"No video files found in input directory\")\n",
        "        exit(1)\n",
        "\n",
        "    # Initialize model once for all videos\n",
        "    model = initialize_model()\n",
        "    if model is None:\n",
        "        print(\"Failed to initialize model\")\n",
        "        exit(1)\n",
        "\n",
        "    # Process each video file\n",
        "    for input_video in input_videos:\n",
        "        base_name = os.path.basename(input_video)\n",
        "        output_video = os.path.join(output_dir, f\"processed_{base_name}\")\n",
        "        try:\n",
        "            process_video(input_video, output_video, model)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {base_name}: {e}\")\n",
        "            continue\n"
      ],
      "metadata": {
        "id": "G-32rhlK7yp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "from google.colab import files  # Ensure this import is correct\n",
        "\n",
        "# Path to the \"output\" folder\n",
        "folder_path = '/content/output'\n",
        "\n",
        "# Create a zip file\n",
        "zip_file_path = '/content/output_files.zip'\n",
        "\n",
        "# Zip the folder\n",
        "with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    for root, dirs, files_in_dir in os.walk(folder_path):\n",
        "        for file in files_in_dir:\n",
        "            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), folder_path))\n",
        "\n",
        "# Download the zip file\n",
        "files.download(zip_file_path)\n"
      ],
      "metadata": {
        "id": "QndYahoVKTVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}